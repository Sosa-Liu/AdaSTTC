{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution Distance Measurement\n",
    "===\n",
    "\n",
    "This is a brief summery about some commonly used distance and similarity measurement in machine learning. Accurate distance measurement is a key block of many machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some necessary packages and modules\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Common distance and similarity measurement\n",
    "\n",
    "### 1.1 Euclidean distance\n",
    "\n",
    "Defined on two vectors (two points in space) : Euclidean distance of point $\\bold x$ and point $\\bold y $  is:\n",
    "$$d_{\\text Euclidean} = \\sqrt {(\\bold x - \\bold y)^\\top (\\bold x - \\bold y)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance: 15.491933384829668\n"
     ]
    }
   ],
   "source": [
    "# Define two matrices\n",
    "matrix1 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=float)\n",
    "matrix2 = torch.tensor([[9, 8, 7], [6, 5, 4], [3, 2, 1]], dtype=float)\n",
    "\n",
    "# Compute the Euclidean distance\n",
    "euclidean_distance = torch.norm(matrix1 - matrix2, p=2)\n",
    "\n",
    "print('Euclidean distance:', euclidean_distance.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Minkowski distance\n",
    "\n",
    "Defined on two vectors (two points in space) : $p$-order Minkowski distance of point $\\bold x$ and point $\\bold y $  is:\n",
    "$$d_{\\text Minkowski} = (||\\bold x - \\bold y||^p)^{\\frac {i}{p}}$$\n",
    "when $p=1$, it is Manhattan distance; $p=2$, it is Euclidean distance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Mahalanobis distance\n",
    "\n",
    "Defined on two vectors (two points in space), and these two points are in the same distribution: Mahalanobis distance of point $\\bold x$ and point $\\bold y $  is:\n",
    "$$d_{Mahalanobis}=\\sqrt {(\\bold x - \\bold y)^\\top \\textstyle \\sum ^{-1}(\\bold x - \\bold y)}$$\n",
    "where $\\sum$ is the covariance matrix of this distribution. \n",
    "When $\\sum = \\bold I$, it is Euclidean distance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cosine similarity\n",
    "\n",
    "Measure the correlation of two vectors (cosine of the Angle between them): Cosine similarity of vector $\\bold x$ and vector $\\bold y $ is:\n",
    "$$\\text cos(\\bold x, \\bold y=\\frac {\\bold x \\cdot \\bold y}{|\\bold x| \\cdot |\\bold y|})$$\n",
    "\n",
    "Cosine distance is widely used in time series data to measure the similarity between two distributions. Given the data $\\bold h_s$ and $\\bold h_t$ , cosine distance is defined as:\n",
    "$$d_{cosine}(\\bold h_s, \\bold h_t)=1-\\frac {\\langle \\bold h_s, \\bold h_t\\rangle}{||\\bold h_s||\\cdot||\\bold h_t||}$$\n",
    "\n",
    "nn.CosineSimilarity()\n",
    "$$ \\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.CosineSimilarity()\n",
    "def cosine(source, target):\n",
    "    # source, target = source.mean(0), target.mean(0)\n",
    "    cos = nn.CosineSimilarity(dim=0)\n",
    "    loss = cos(source, target)\n",
    "    return loss.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mutual information\n",
    "\n",
    "Defined on two distributions $X$ and $Y$, mutual information of them is:\n",
    "$$I(X,Y)=\\sum_{x \\in X}\\sum_{y \\in Y}p(x,y)log \\frac {p(x,y)}{p(x)p(y)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation coefficient\n",
    "\n",
    "### 4.1 Pearson's correlation coefficient\n",
    "\n",
    "Measure the correlation between two random variables $X$ and $Y$ï¼š\n",
    "$$\\rho _{X,Y}=\\frac {Cov(X,Y)}{\\sigma _X \\sigma _Y}$$\n",
    "where $Cov(X,Y)$ denotes the covariance matrix, $\\sigma$ is the standard deviation.\n",
    "Pearson's correlation coefficient ranges [-1, 1], and a greater absolute value indicates a bigger correlation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Jaccard correlation coefficient\n",
    "\n",
    "Measure the correlation between two sets $X$ and $Y$ï¼š\n",
    "$$J=\\frac{X \\cap Y}{X \\cup Y}$$\n",
    "Jaccard distance:\n",
    "$$d_{\\text Jaccard}=1-J$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Kullback-Leibler divergence and Jensen-Shannon Divergence\n",
    "\n",
    "**Kullback-Leibler divergence** measures the distance between two probability distributions $P(x)$ and $Q(x)$:\n",
    "$$D_{KL}(P||Q)=\\sum_{i=1}P(x)log\\frac {P(x)}{Q(x)}$$\n",
    "This is an asymmetric distance: $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$\n",
    "\n",
    "The basic idea is to compare the difference in probability between two distributions on the same event. The smaller the KL divergence, the more similar the two distributions are.\n",
    "\n",
    "`torch.nn.KLDivloss()`: The Kullback-Leibler divergence loss.\n",
    "\n",
    "For tensors of the same shape $y_{\\text{pred}}$, $y_{\\text{true}}$, where $y_{\\text{pred}}$ is the `input` and $y_{\\text{true}}$ is the `target`, we define the **pointwise KL-divergence** as\n",
    "$$L(y_{\\text{pred}},\\ y_{\\text{true}})\n",
    "    = y_{\\text{true}} \\cdot \\log \\frac{y_{\\text{true}}}{y_{\\text{pred}}}\n",
    "    = y_{\\text{true}} \\cdot (\\log y_{\\text{true}} - \\log y_{\\text{pred}})$$\n",
    "To avoid underflow issues when computing this quantity, this loss expects the argument `input` in the log-space.\n",
    "\n",
    "**Jensen-Shannon Divergence** is the mean of KL divergence, and it is symmetric and non-negative. The smaller the JS divergence, the more similar the two distributions are.\n",
    "$$JSP(P||Q)=\\frac {1}{2}D_{KL}(P||M)+\\frac {1}{2}D_{KL}(Q||M)$$\n",
    "where $M=\\frac {1}{2}(P+Q)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.KLDivLoss()\n",
    "def kl_div(source, target):\n",
    "    if len(source) < len(target):\n",
    "        target = target[:len(source)]\n",
    "    elif len(source) > len(target):\n",
    "        source = source[:len(target)]\n",
    "    criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "    loss = criterion(source.log(), target)\n",
    "    return loss\n",
    "\n",
    "def js(source, target):\n",
    "    if len(source) < len(target):\n",
    "        target = target[:len(source)]\n",
    "    elif len(source) > len(target):\n",
    "        source = source[:len(target)]\n",
    "    M = .5 * (source + target)\n",
    "    loss_1, loss_2 = kl_div(source, M), kl_div(target, M)\n",
    "    return .5 * (loss_1 + loss_2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Maximum Mean Discrepancy(MMD)\n",
    "\n",
    "The MMD is a non-parametric distance between two probability distributions. MMD measures the distance of the mean embeddings of the samples from two distributions in a Reproducing Kernel Hilbert Space (RKHS). MMD can be empirically estimated by\n",
    "$$d_{mmd}(\\bold h_s, \\bold h_t)=\\frac {1}{n_s^2}\\sum^{n_s}_{i,j=1}k(h_{s_i},h_{s_j})+\\frac {1}{n_t^2}\\sum_{i,j=n_s+1}^{n_s+n_t}k(h_{t_i},h_{t_j})-\\frac {2}{n_s n_t}\\sum_{i=1}^{n_s}\\sum_{j=n_s+1}^{n_s+n_t}k(h_{s_i},h_{t_j})\\sum_{i=n_s+1}^{n_s+n_t}\\sum_{j=1}^{n_s}k(h_{s_i},h_{t_j})$$\n",
    "where $k(\\cdot,\\cdot)$ is the kernel function such as RBF kernel and linear kernel, and $n_s=|h_s|$, $n_t=|h_t|$ are the number of the data from two distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMD_loss(nn.Module):\n",
    "    def __init__(self, kernel_type='linear', kernel_mul=2.0, kernel_num=5):\n",
    "        super(MMD_loss, self).__init__()\n",
    "        self.kernel_num = kernel_num\n",
    "        self.kernel_mul = kernel_mul\n",
    "        self.fix_sigma = None\n",
    "        self.kernel_type = kernel_type\n",
    "\n",
    "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
    "        n_samples = int(source.size()[0]) + int(target.size()[0])   # n_s + n_t\n",
    "        total = torch.cat([source, target], dim=0)  # (n_samples, d)\n",
    "        total0 = total.unsqueeze(0).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))     # (1, n_samples, d) -> (n_samples, n_samples, d)\n",
    "        total1 = total.unsqueeze(1).expand(\n",
    "            int(total.size(0)), int(total.size(0)), int(total.size(1)))     # (n_samples, 1, d) -> (n_samples, n_samples, d)\n",
    "        L2_distance = ((total0-total1)**2).sum(2)       # (n_samples, n_samples)    (x_i - x_j)**2\n",
    "        if fix_sigma:\n",
    "            bandwidth = fix_sigma\n",
    "        else:\n",
    "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
    "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
    "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
    "                          for i in range(kernel_num)]        # len() = n_samples    \n",
    "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
    "                      for bandwidth_temp in bandwidth_list]     # len() = n_samples  kernel_val[i].shape:(n_samples, n_samples)\n",
    "        return sum(kernel_val)      # (n_samples, n_samples)\n",
    "\n",
    "    def linear_mmd(self, X, Y):\n",
    "        delta = X.mean(axis=0) - Y.mean(axis=0)\n",
    "        loss = delta.dot(delta.T)\n",
    "        return loss\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        if self.kernel_type == 'linear':\n",
    "            return self.linear_mmd(source, target)\n",
    "        elif self.kernel_type == 'rbf':\n",
    "            batch_size = int(source.size()[0])  # n_s\n",
    "            kernels = self.guassian_kernel(\n",
    "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
    "            with torch.no_grad():\n",
    "                XX = torch.mean(kernels[:batch_size, :batch_size])\n",
    "                YY = torch.mean(kernels[batch_size:, batch_size:])\n",
    "                XY = torch.mean(kernels[:batch_size, batch_size:])\n",
    "                YX = torch.mean(kernels[batch_size:, :batch_size])\n",
    "                loss = torch.mean(XX + YY - XY - YX)\n",
    "            return loss\n",
    "\n",
    "def mmd(source, target, kernel_type='linear', kernel_mul=2.0, kernel_num=5):\n",
    "    model = MMD_loss(kernel_type, kernel_mul, kernel_num)\n",
    "    return model(source, target)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adversarial Discrepancy\n",
    "\n",
    "The domain discrepancy can be parameterized as a neural network and it is referred to as the domain adversarial discrepancy[1]. An additional network named domain discriminator denoted by ð· is introduced. The domain adversarial objective is defined as: \n",
    "\n",
    "$$l_{adv}(\\bold h_s, \\bold h_t) = \\mathbb E[log[D(\\bold h_s)]] + \\mathbb E[log[1-D(\\bold h_t)]]$$\n",
    "\n",
    "where $\\mathbb E$ denotes expectation. Hence, the adversarial discrepancy is \n",
    "$$d_{adv}(\\bold h_s, \\bold h_t) = -l_{adv}(\\bold h_s, \\bold h_t)$$\n",
    "\n",
    "\n",
    "`[1] Yaroslav Ganin, E. Ustinova, Hana Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky. 2016. Domain-Adversarial Training of Neural Networks. JMLR 17 (2016), 59:1â€“59:35`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''two layer net: affine - > relu - > affine - > sigmoid'''\n",
    "    def __init__(self, input_dim=256, hidden_dim=256):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dis1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dis2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dis1(x))\n",
    "        x = self.dis2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "# nn.BCELoss()\n",
    "def adv(source, target, input_dim=256, hidden_dim=512):\n",
    "    domain_loss = nn.BCELoss()\n",
    "    adv_net = Discriminator(input_dim, hidden_dim).cuda()\n",
    "    domain_src = torch.ones((len(source), 1)).cuda()     # labels of source domain 1\n",
    "    domain_tar = torch.zeros((len(target), 1)).cuda()    # labels of target domain 0\n",
    "    reverse_src = ReverseLayerF.apply(source, 1)        # ???\n",
    "    reverse_tar = ReverseLayerF.apply(target, 1)\n",
    "    pred_src = adv_net(reverse_src)\n",
    "    pred_tar = adv_net(reverse_tar)\n",
    "    loss_s, loss_t = domain_loss(\n",
    "        pred_src, domain_src), domain_loss(pred_tar, domain_tar)\n",
    "    loss = loss_s + loss_t\n",
    "    return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Correlation Alignment(CORAL)\n",
    "\n",
    "The CORAL distance is defined as the distance of the second-order statistic (covariance) of the samples from two distributions:\n",
    "$$d_{coral}(\\bold h_s, \\bold h_t) = \\frac {1}{4q^2}||\\bold C_s-C_t||_F^2$$\n",
    "\n",
    "where $q$ is the dimension of the two distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CORAL(source, target):\n",
    "    d = source.size(1)  # feature dimension\n",
    "    ns, nt = source.size(0), target.size(0)     # number of samples\n",
    "\n",
    "    # source covariance\n",
    "    tmp_s = torch.ones((1, ns)).cuda() @ source     # torch.Size: ([1,d])\n",
    "    cs = (source.t() @ source - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)    # torch.Size: ([d,d])\n",
    "\n",
    "    # target covariance\n",
    "    tmp_t = torch.ones((1, nt)).cuda() @ target\n",
    "    ct = (target.t() @ target - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)\n",
    "\n",
    "    # frobenius norm\n",
    "    loss = (cs - ct).pow(2).sum()\n",
    "    loss = loss / (4 * d * d)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mine(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=512):\n",
    "        super(Mine, self).__init__()\n",
    "        self.fc1_x = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc1_y = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        h1 = F.leaky_relu(self.fc1_x(x)+self.fc1_y(y))\n",
    "        h2 = self.fc2(h1)\n",
    "        return h2\n",
    "\n",
    "\n",
    "class Mine_estimator(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=512):\n",
    "        super(Mine_estimator, self).__init__()\n",
    "        self.mine_model = Mine(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        Y_shuffle = Y[torch.randperm(len(Y))]\n",
    "        loss_joint = self.mine_model(X, Y)\n",
    "        loss_marginal = self.mine_model(X, Y_shuffle)\n",
    "        ret = torch.mean(loss_joint) - \\\n",
    "            torch.log(torch.mean(torch.exp(loss_marginal)))\n",
    "        loss = ret if ret == 0 else -ret\n",
    "        return loss\n",
    "\n",
    "def mine(source, target, input_dim=2048, hidden_dim=512):\n",
    "    model = Mine_estimator(input_dim, hidden_dim).cuda()\n",
    "    return model(source, target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_dist(X, Y):\n",
    "    n, d = X.shape\n",
    "    m, _ = Y.shape\n",
    "    assert d == Y.shape[1]\n",
    "    a = X.unsqueeze(1).expand(n, m, d)\n",
    "    b = Y.unsqueeze(0).expand(n, m, d)\n",
    "    return torch.pow(a - b, 2).sum((0, 1, 2))\n",
    "\n",
    "\n",
    "def pairwise_dist_np(X, Y):\n",
    "    n, d = X.shape\n",
    "    m, _ = Y.shape\n",
    "    assert d == Y.shape[1]\n",
    "    a = np.expand_dims(X, 1)\n",
    "    b = np.expand_dims(Y, 0)\n",
    "    a = np.tile(a, (1, m, 1))\n",
    "    b = np.tile(b, (n, 1, 1))\n",
    "    return np.power(a - b, 2).sum((0, 1, 2))\n",
    "\n",
    "\n",
    "def pa(X, Y):\n",
    "    XY = np.dot(X, Y.T)\n",
    "    XX = np.sum(np.square(X), axis=1)\n",
    "    XX = np.transpose([XX])\n",
    "    YY = np.sum(np.square(Y), axis=1)\n",
    "    dist = XX + YY - 2 * XY\n",
    "\n",
    "    return dist.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(124)\n",
    "source = torch.rand((2,5),device='cuda')\n",
    "target = torch.rand((2,5),device='cuda')\n",
    "# source = torch.tensor([[1.,2.,3.,4.,5.],[6.,7.,8.,9.,10.]], device='cuda')\n",
    "# target = torch.tensor([[1.,2.,3.,4.,5.],[6.,7.,8.,9.,10.]], device='cuda')\n",
    "loss_adv = adv(source,target, 5, 16)\n",
    "loss_coral = CORAL(source, target)\n",
    "loss_cosine = cosine(source, target)\n",
    "loss_js = js(source, target)\n",
    "loss_kl = kl_div(source, target)\n",
    "loss_mmd = mmd(source, target, 'rbf')\n",
    "loss_mine = mine(source, target, 5, 10)\n",
    "loss_pa = pairwise_dist(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
